---
---
@inproceedings{li-etal-2020-linguistically,
    title = "Linguistically-Informed Transformations ({LIT}): A Method for Automatically Generating Contrast Sets",
    author = "Li, Chuanrong*  and
      Shengshuo, Lin*  and
      Liu, Zeyu*  and
      Wu, Xinyi*  and
      Zhou, Xuhui*  and
      Steinert-Threlkeld, Shane",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://www.aclweb.org/anthology/2020.blackboxnlp-1.12",
    pages = "126--135",
    abstract = "Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often requires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models{'} performance on the contrast sets by applying LIT to augment the training data, without affecting performance on the original data.",
}
@inproceedings{zhou-etal-2020-multilevel,
    title = "Multilevel Text Alignment with Cross-Document Attention",
    author = "Zhou, Xuhui  and
      Pappas, Nikolaos  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://www.aclweb.org/anthology/2020.emnlp-main.407",
    pages = "5012--5025",
    abstract = "Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence \textit{and} document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.",
}
@inproceedings{zhou2020rpd,
    title={RPD: A Distance Function Between Word Embeddings},
    author={Xuhui Zhou and Zaixiang Zheng and Shujian Huang},
    year={2020},
    eprint={2005.08113},
    archivePrefix={arXiv},
    html={https://www.semanticscholar.org/paper/Evaluating-Commonsense-in-Pre-trained-Language-ZHOU-Zhang/01f2b214962997260020279bd1fd1f8f372249d4},
    primaryClass={cs.CL},
    booktitle="Proceedings of ACL SRW"
}

@inproceedings{ZHOU2019EvaluatingCI,
  title={Evaluating Commonsense in Pre-trained Language Models},
  author={Xuhui Zhou and Y. Zhang and Leyang Cui and Dandan Huang},
  html={https://www.semanticscholar.org/paper/RPD%3A-A-Distance-Function-Between-Word-Embeddings-ZHOU-Zheng/9eeaed019d408e81dcd5ff2c820fc96357e01150},
  year={2020},
  volume={abs/1911.11931},
  booktitle="Proceedings of AAAI"
}

